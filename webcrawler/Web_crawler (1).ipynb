{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPUc6966ulkc",
        "outputId": "d668611e-e35f-4d38-d9f1-a7396a4482f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling page: https://books.toscrape.com/catalogue/page-1.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-2.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-3.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-4.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-5.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-6.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-7.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-8.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-9.html\n",
            "Crawling page: https://books.toscrape.com/catalogue/page-10.html\n",
            "Saved 200 products to books.csv\n"
          ]
        }
      ],
      "source": [
        "# simple_crawler.py\n",
        "import time\n",
        "import csv\n",
        "import requests\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from urllib import robotparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "BASE_URL = \"https://books.toscrape.com/\"\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"MyCrawler/1.0 (+https://example.com/contact) - educational use\"\n",
        "}\n",
        "\n",
        "# polite crawler: check robots.txt\n",
        "def can_fetch(url, user_agent=HEADERS[\"User-Agent\"]):\n",
        "    rp = robotparser.RobotFileParser()\n",
        "    robots_url = urljoin(url, \"/robots.txt\")\n",
        "    rp.set_url(robots_url)\n",
        "    try:\n",
        "        rp.read()\n",
        "    except Exception:\n",
        "        # if robots.txt can't be fetched, default to false to be safe or True if you prefer\n",
        "        return False\n",
        "    return rp.can_fetch(user_agent, url)\n",
        "\n",
        "def get_soup(url):\n",
        "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
        "    resp.raise_for_status()\n",
        "    return BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "def parse_product_card(card, base_url):\n",
        "    # Example for books.toscrape structure — change selectors for your target site\n",
        "    title_tag = card.select_one(\"h3 a\")\n",
        "    title = title_tag[\"title\"].strip()\n",
        "    relative_link = title_tag[\"href\"]\n",
        "    product_url = urljoin(base_url, relative_link)\n",
        "\n",
        "    price = card.select_one(\".price_color\").get_text(strip=True)\n",
        "    availability = card.select_one(\".availability\").get_text(strip=True)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"price\": price,\n",
        "        \"availability\": availability,\n",
        "        \"product_url\": product_url\n",
        "    }\n",
        "\n",
        "def crawl(start_url, max_pages=5, delay=1.0, output_csv=\"products.csv\"):\n",
        "    # Respect robots.txt for the start URL domain\n",
        "    parsed = urlparse(start_url)\n",
        "    domain_root = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
        "    if not can_fetch(domain_root):\n",
        "        raise SystemExit(f\"Robots.txt disallows crawling {domain_root} for this user-agent.\")\n",
        "\n",
        "    products = []\n",
        "    next_page = start_url\n",
        "    pages_crawled = 0\n",
        "\n",
        "    while next_page and pages_crawled < max_pages:\n",
        "        print(f\"Crawling page: {next_page}\")\n",
        "        soup = get_soup(next_page)\n",
        "\n",
        "        # Find all product cards — change this selector to match the site\n",
        "        cards = soup.select(\".product_pod\")\n",
        "        for card in cards:\n",
        "            try:\n",
        "                prod = parse_product_card(card, domain_root)\n",
        "                products.append(prod)\n",
        "            except Exception as e:\n",
        "                print(\"Failed to parse product card:\", e)\n",
        "\n",
        "        # Find \"next\" link (example specific to books.toscrape)\n",
        "        next_tag = soup.select_one(\".next a\")\n",
        "        if next_tag:\n",
        "            next_page = urljoin(next_page, next_tag[\"href\"])\n",
        "        else:\n",
        "            next_page = None\n",
        "\n",
        "        pages_crawled += 1\n",
        "        time.sleep(delay)  # polite delay\n",
        "\n",
        "    # Save results\n",
        "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"availability\", \"product_url\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(products)\n",
        "\n",
        "    print(f\"Saved {len(products)} products to {output_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # start crawling from catalog page\n",
        "    start = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
        "    crawl(start, max_pages=10, delay=1.0, output_csv=\"books.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VbIQcixhvmwy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}